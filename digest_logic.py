# digest_logic.py

import os
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv  # <-- Ð£Ð±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ ÑÑ‚Ð° ÑÑ‚Ñ€Ð¾ÐºÐ° ÐµÑÑ‚ÑŒ
import json

# --- Configuration ---
# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¸Ð· .env Ñ„Ð°Ð¹Ð»Ð° Ð² Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ðµ ÐžÐ¡ Ð´Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑÐºÑ€Ð¸Ð¿Ñ‚Ð°
load_dotenv() 

# Ð¢ÐµÐ¿ÐµÑ€ÑŒ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡Ñ‚Ð¾ Ð±Ñ‹Ð»Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
SEARCH_ENGINE_ID = os.getenv("SEARCH_ENGINE_ID")

# ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ñ‡Ñ‚Ð¾ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ð»Ð¸ÑÑŒ
if not GOOGLE_API_KEY or not SEARCH_ENGINE_ID:
    print("âŒ ÐžÐ¨Ð˜Ð‘ÐšÐ: GOOGLE_API_KEY Ð¸Ð»Ð¸ SEARCH_ENGINE_ID Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð² .env Ñ„Ð°Ð¹Ð»Ðµ.")
    print("   ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, ÑƒÐ±ÐµÐ´Ð¸Ñ‚ÐµÑÑŒ, Ñ‡Ñ‚Ð¾ Ñ„Ð°Ð¹Ð» .env ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚ Ð¸ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ Ð¾Ð±Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ.")
    # ÐœÐ¾Ð¶Ð½Ð¾ Ð»Ð¸Ð±Ð¾ Ð·Ð°Ð²ÐµÑ€ÑˆÐ¸Ñ‚ÑŒ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ, Ð»Ð¸Ð±Ð¾ Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ, Ð½Ð¾ ÑÐºÑ€Ð¸Ð¿Ñ‚ ÑƒÐ¿Ð°Ð´ÐµÑ‚ Ð¿Ð¾Ð·Ð¶Ðµ
    # exit() # Ð Ð°ÑÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ, ÐµÑÐ»Ð¸ Ñ…Ð¾Ñ‚Ð¸Ñ‚Ðµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ ÑÐºÑ€Ð¸Ð¿Ñ‚ ÑÑ€Ð°Ð·Ñƒ Ð¾ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°Ð»ÑÑ

SEARCH_QUERY = "digital fraud and cybercrime news"
NUM_RESULTS_TO_FETCH = 10
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
OUTPUT_PROMPT_FILE = "final_prompt_for_ai.txt"

def search_articles():
    """Searches Google for recent articles based on the query."""
    print(f"ðŸ” Searching for articles with query: '{SEARCH_QUERY}'...")
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð¼
    if not GOOGLE_API_KEY or not SEARCH_ENGINE_ID:
        print("âŒ ÐÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº: Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ API Ð½Ðµ ÑÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹.")
        return []

    url = "https://www.googleapis.com/customsearch/v1"
    params = {'key': GOOGLE_API_KEY, 'cx': SEARCH_ENGINE_ID, 'q': SEARCH_QUERY, 'num': NUM_RESULTS_TO_FETCH, 'sort': 'date'}
    try:
        response = requests.get(url, params=params)
        response.raise_for_status()
        search_results = response.json()
        if 'items' not in search_results:
            print("âŒ Error: No articles found. Check API key/search engine ID in your .env file.")
            # Ð”Ð¾Ð±Ð°Ð²Ð¸Ð¼ Ð²Ñ‹Ð²Ð¾Ð´ Ð¾ÑˆÐ¸Ð±ÐºÐ¸ Ð¾Ñ‚ Google, ÐµÑÐ»Ð¸ Ð¾Ð½ ÐµÑÑ‚ÑŒ
            if 'error' in search_results:
                print(f"   Google API Error: {search_results['error']['message']}")
            return []
        articles = [{'title': item['title'], 'url': item['link'], 'snippet': item.get('snippet', '')} for item in search_results['items']]
        print(f"âœ… Found {len(articles)} potential articles.")
        return articles
    except requests.exceptions.HTTPError as http_err:
        print(f"âŒ HTTP Request failed: {http_err}")
        # ÐŸÐ¾Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð²Ñ‹Ð²ÐµÑÑ‚Ð¸ Ñ‚ÐµÐ»Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°, ÐµÑÐ»Ð¸ ÑÑ‚Ð¾ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° (4xx) Ð¸Ð»Ð¸ ÑÐµÑ€Ð²ÐµÑ€Ð° (5xx)
        try:
            error_details = http_err.response.json()
            print("   Error Details from API:")
            print(json.dumps(error_details, indent=2))
        except json.JSONDecodeError:
            print("   Could not parse error response from API.")
        return []
    except requests.exceptions.RequestException as e:
        print(f"âŒ A network error occurred: {e}")
        return []

# ... (Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð¹ ÐºÐ¾Ð´ Ð² digest_logic.py Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ð±ÐµÐ· Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¹) ...

def get_article_content(url):
    """Scrapes the main text content from a given article URL."""
    try:
        print(f"   - Reading article: {url}")
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        paragraphs = soup.find_all('p')
        text_content = "\n".join([p.get_text() for p in paragraphs])
        return text_content if text_content else "Could not extract content."
    except requests.exceptions.RequestException as e:
        print(f"   - â—ï¸ Failed to fetch article: {e}")
        return "Failed to fetch article content."

def generate_final_prompt(articles_data):
    """Generates a structured prompt for the AI based on the scraped content."""
    # ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð° Ð±Ñ‹Ð» ÑƒÐ±Ñ€Ð°Ð½ Ð´Ð»Ñ ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ‚Ð¸, Ð¾Ð½ Ð¾ÑÑ‚Ð°ÐµÑ‚ÑÑ Ñ‚Ð°ÐºÐ¸Ð¼ Ð¶Ðµ
    prompt_header = "You are an expert AI analyst... (rest of your detailed prompt)"
    prompt_body = ""
    for i, article in enumerate(articles_data, 1):
        prompt_body += f"\n--- ARTICLE {i} ---\nURL: {article['url']}\nTITLE: {article['title']}\nCONTENT:\n{article['content']}\n"
    prompt_footer = "--- END OF ARTICLE DATA --- (rest of your detailed template)"
    
    final_prompt = prompt_header + prompt_body + prompt_footer
    with open(OUTPUT_PROMPT_FILE, "w", encoding="utf-8") as f:
        f.write(final_prompt)
    print(f"\nðŸš€ Success! The final prompt has been saved to '{OUTPUT_PROMPT_FILE}'.")
    